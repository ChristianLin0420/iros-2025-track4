# InternVL3-1B Cross-Modal Retrieval Methodology

## Overview

This methodology replaces the baseline's separate BERT text encoder and Swin vision encoder with **InternVL3-1B**, a unified Vision-Language Model that provides better cross-modal alignment and understanding capabilities.

## Key Improvements

1. **Unified VLM Architecture**: Uses InternVL3-1B's integrated vision and text encoders
2. **Better Cross-Modal Alignment**: Pre-trained on large-scale vision-language data
3. **Enhanced Spatial Understanding**: Improved spatial relation modeling
4. **Efficient Training**: Adapter-based fine-tuning approach
5. **Comprehensive Logging**: WandB integration for experiment tracking

## Architecture

### Comprehensive Model Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     INPUT PROCESSING                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Image: [B, 3, 384, 384]                    Text: [B, max_tokens]                      â”‚
â”‚  â†“ Preprocessing                            â†“ Tokenization                              â”‚
â”‚  â€¢ Resize to 384x384                        â€¢ BERT Tokenizer                            â”‚
â”‚  â€¢ Normalize RGB                            â€¢ Padding to max_tokens                     â”‚
â”‚  â€¢ Tensor conversion                        â€¢ Attention masks                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                INTERNVL3-1B BASE MODEL                                 â”‚
â”‚                                (1B Parameters - FROZEN)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚        VISION PATH          â”‚            â”‚          TEXT PATH          â”‚           â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚            â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚           â”‚
â”‚  â”‚   â”‚    ViT-Large-336    â”‚   â”‚            â”‚   â”‚     LLaMA-2-7B      â”‚   â”‚           â”‚
â”‚  â”‚   â”‚                     â”‚   â”‚            â”‚   â”‚                     â”‚   â”‚           â”‚
â”‚  â”‚   â”‚ â€¢ Patch: 14x14      â”‚   â”‚            â”‚   â”‚ â€¢ Vocab: 32000      â”‚   â”‚           â”‚
â”‚  â”‚   â”‚ â€¢ Layers: 24        â”‚   â”‚            â”‚   â”‚ â€¢ Layers: 32        â”‚   â”‚           â”‚
â”‚  â”‚   â”‚ â€¢ Heads: 16         â”‚   â”‚            â”‚   â”‚ â€¢ Heads: 32         â”‚   â”‚           â”‚
â”‚  â”‚   â”‚ â€¢ Hidden: 1024      â”‚   â”‚            â”‚   â”‚ â€¢ Hidden: 4096      â”‚   â”‚           â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚            â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚              â†“                                           â†“                            â”‚
â”‚  Vision Features: [B, 576, 1024]            Text Features: [B, max_tokens, 896]      â”‚
â”‚  â€¢ 576 = (384/14)Â² patches                  â€¢ Dynamic dimension detected              â”‚
â”‚  â€¢ 1024d per patch                          â€¢ 896d per token (detected)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DYNAMIC ADAPTER LAYERS                                    â”‚
â”‚                             (Trainable Parameters: ~528K)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚      VISION ADAPTER         â”‚            â”‚       TEXT ADAPTER          â”‚           â”‚
â”‚  â”‚                             â”‚            â”‚                             â”‚           â”‚
â”‚  â”‚  Input: [B, 576, 1024]      â”‚            â”‚  Input: [B, max_tokens, 896]â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  Linear(1024 â†’ 256)         â”‚            â”‚  Linear(896 â†’ 256)          â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  ReLU + Dropout(0.1)        â”‚            â”‚  ReLU + Dropout(0.1)        â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  Output: [B, 576, 256]      â”‚            â”‚  Output: [B, max_tokens, 256]â”‚           â”‚
â”‚  â”‚                             â”‚            â”‚                             â”‚           â”‚
â”‚  â”‚  Trainable Params: 262K     â”‚            â”‚  Trainable Params: 230K     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                         â”‚
â”‚  # Feature Extraction (CLS tokens)                                                     â”‚
â”‚  Vision Embedding: [B, 256]  â†â”€â”€ vision_features[:, 0, :]                             â”‚
â”‚  Text Embedding: [B, 256]    â†â”€â”€ text_features[:, 0, :]                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                CROSS-MODAL FUSION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚     VISION PROJECTOR        â”‚            â”‚      TEXT PROJECTOR         â”‚           â”‚
â”‚  â”‚                             â”‚            â”‚                             â”‚           â”‚
â”‚  â”‚  Input: [B, 256]            â”‚            â”‚  Input: [B, 256]            â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  Linear(256 â†’ 256)          â”‚            â”‚  Linear(256 â†’ 256)          â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  L2 Normalization           â”‚            â”‚  L2 Normalization           â”‚           â”‚
â”‚  â”‚        â†“                    â”‚            â”‚        â†“                    â”‚           â”‚
â”‚  â”‚  Vision Proj: [B, 256]      â”‚            â”‚  Text Proj: [B, 256]        â”‚           â”‚
â”‚  â”‚                             â”‚            â”‚                             â”‚           â”‚
â”‚  â”‚  Trainable Params: 65K      â”‚            â”‚  Trainable Params: 65K      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                         â”‚
â”‚  # Concatenated Features for Multi-task Heads                                          â”‚
â”‚  Fused Features: [B, 512] â†â”€â”€ torch.cat([vision_proj, text_proj], dim=1)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              MULTI-TASK PREDICTION HEADS                               â”‚
â”‚                             (Trainable Parameters: ~36K)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   ITM HEAD          â”‚  â”‚   BBOX HEAD         â”‚  â”‚  SPATIAL HEAD       â”‚           â”‚
â”‚  â”‚                     â”‚  â”‚                     â”‚  â”‚                     â”‚           â”‚
â”‚  â”‚ Input: [B, 512]     â”‚  â”‚ Input: [B, 512]     â”‚  â”‚ Input: [B, 512]     â”‚           â”‚
â”‚  â”‚       â†“             â”‚  â”‚       â†“             â”‚  â”‚       â†“             â”‚           â”‚
â”‚  â”‚ Linear(512 â†’ 2)     â”‚  â”‚ Linear(512 â†’ 4)     â”‚  â”‚ Linear(512 â†’ 6)     â”‚           â”‚
â”‚  â”‚       â†“             â”‚  â”‚       â†“             â”‚  â”‚       â†“             â”‚           â”‚
â”‚  â”‚ Match Logits: [B,2] â”‚  â”‚ BBox Coords: [B,4]  â”‚  â”‚ Spatial Rels: [B,6] â”‚           â”‚
â”‚  â”‚ â€¢ [No Match, Match] â”‚  â”‚ â€¢ [x1, y1, x2, y2]  â”‚  â”‚ â€¢ [left, right,     â”‚           â”‚
â”‚  â”‚                     â”‚  â”‚                     â”‚  â”‚   above, below,     â”‚           â”‚
â”‚  â”‚ Params: 1K          â”‚  â”‚ Params: 2K          â”‚  â”‚   inside, outside]  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Params: 3K          â”‚           â”‚
â”‚                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                         â”‚
â”‚  # Contrastive Learning (ITC)                                                          â”‚
â”‚  Vision Proj: [B, 256] â”€â”€â”                                                             â”‚
â”‚  Text Proj: [B, 256]   â”€â”€â”¼â”€â”€â†’ Similarity Matrix: [B, B]                               â”‚
â”‚  Learnable Temp: 0.07 â”€â”€â”˜    â†“                                                        â”‚
â”‚                              Contrastive Loss (InfoNCE)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  LOSS COMPUTATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                            LOSS COMPONENTS                                          â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  1. ITC Loss (Image-Text Contrastive):                                             â”‚ â”‚
â”‚  â”‚     â€¢ InfoNCE loss on similarity matrix                                            â”‚ â”‚
â”‚  â”‚     â€¢ Weight: 1.0                                                                  â”‚ â”‚
â”‚  â”‚     â€¢ Formula: -log(exp(sim_i2t) / Î£ exp(sim_all))                                â”‚ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  2. ITM Loss (Image-Text Matching):                                                â”‚ â”‚
â”‚  â”‚     â€¢ Binary cross-entropy on match predictions                                    â”‚ â”‚
â”‚  â”‚     â€¢ Weight: 1.0                                                                  â”‚ â”‚
â”‚  â”‚     â€¢ Formula: BCE(match_logits, match_targets)                                    â”‚ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  3. BBox Loss (Bounding Box Regression):                                           â”‚ â”‚
â”‚  â”‚     â€¢ L1 loss on bounding box coordinates                                          â”‚ â”‚
â”‚  â”‚     â€¢ Weight: 0.1                                                                  â”‚ â”‚
â”‚  â”‚     â€¢ Formula: L1(bbox_pred, bbox_target)                                          â”‚ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  4. Spatial Loss (Spatial Relation Classification):                                â”‚ â”‚
â”‚  â”‚     â€¢ Cross-entropy on spatial relation predictions                                â”‚ â”‚
â”‚  â”‚     â€¢ Weight: 0.1                                                                  â”‚ â”‚
â”‚  â”‚     â€¢ Formula: CE(spatial_logits, spatial_targets)                                 â”‚ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  Total Loss = ITC_loss + ITM_loss + 0.1*BBox_loss + 0.1*Spatial_loss             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                          TRAINING PARAMETERS                                        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚                                                                                     â”‚ â”‚
â”‚  â”‚  â€¢ Optimizer: AdamW (lr=1e-4, weight_decay=0.01)                                   â”‚ â”‚
â”‚  â”‚  â€¢ Scheduler: Linear with warmup (0.1 ratio)                                       â”‚ â”‚
â”‚  â”‚  â€¢ Batch Size: 4 (optimized for A40 GPU)                                           â”‚ â”‚
â”‚  â”‚  â€¢ Epochs: 5                                                                       â”‚ â”‚
â”‚  â”‚  â€¢ Precision: Float32 (for stable adapter training)                                â”‚ â”‚
â”‚  â”‚  â€¢ Gradient Accumulation: Dynamic based on memory                                  â”‚ â”‚
â”‚  â”‚  â€¢ Only Adapters + Task Heads are trainable (~528K params)                        â”‚ â”‚
â”‚  â”‚  â€¢ Base InternVL3-1B model is frozen (1B params)                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technical Pipeline Details

#### 1. **Dynamic Dimension Detection**
- Vision features: Automatically detected as 1024-dimensional
- Text features: Automatically detected as 896-dimensional (not 1024 as initially assumed)
- Adapters dynamically created based on actual feature dimensions

#### 2. **Parameter Efficiency**
```
Total Parameters: ~1.528B
â”œâ”€â”€ Frozen InternVL3-1B: 1B parameters (99.97%)
â””â”€â”€ Trainable Components: ~528K parameters (0.03%)
    â”œâ”€â”€ Vision Adapter: 262K params (1024Ã—256 + 256)
    â”œâ”€â”€ Text Adapter: 230K params (896Ã—256 + 256)
    â”œâ”€â”€ Vision Projector: 65K params (256Ã—256 + 256)
    â”œâ”€â”€ Text Projector: 65K params (256Ã—256 + 256)
    â”œâ”€â”€ ITM Head: 1K params (512Ã—2 + 2)
    â”œâ”€â”€ BBox Head: 2K params (512Ã—4 + 4)
    â””â”€â”€ Spatial Head: 3K params (512Ã—6 + 6)
```

#### 3. **Memory Optimization Strategy**
- **Float32 Precision**: Ensures stable gradient flow through adapters
- **Gradient Checkpointing**: Reduces memory usage during backpropagation
- **Batch Size 4**: Optimized for A40 GPU memory constraints
- **Frozen Base Model**: Significantly reduces memory requirements

#### 4. **Multi-Task Learning Approach**
- **Primary Tasks**: Image-Text Contrastive (ITC) + Image-Text Matching (ITM)
- **Auxiliary Tasks**: Bounding Box Regression + Spatial Relation Classification
- **Loss Weighting**: Balanced approach with reduced weights for auxiliary tasks

#### 5. **Training Stability Features**
- **Dynamic Adapter Creation**: Handles varying feature dimensions
- **Comprehensive Error Handling**: Robust to spatial loss computation failures
- **Gradient Flow Management**: Proper module registration and in-place operation handling
- **WandB Integration**: Real-time monitoring and logging

## ğŸ§ª Evaluation Commands

### Original InternVL3 Single-Stage Training

#### Training
```bash
# Standard training
python internvl3_bbox.py \
    --config configs/internvl3_bbox.yaml \
    --output_dir ./output/internvl3_single_stage \
    --use_wandb

# Training with custom batch size
python internvl3_bbox.py \
    --config configs/internvl3_bbox.yaml \
    --output_dir ./output/internvl3_single_stage \
    --bs 8 \
    --use_wandb
```

#### Evaluation
```bash
# Evaluate trained model
python internvl3_bbox.py \
    --config configs/internvl3_bbox.yaml \
    --checkpoint ./output/internvl3_single_stage/checkpoint_4.pth \
    --evaluate \
    --output_dir ./output/internvl3_eval

# Evaluation with WandB logging
python internvl3_bbox.py \
    --config configs/internvl3_bbox.yaml \
    --checkpoint ./output/internvl3_single_stage/checkpoint_4.pth \
    --evaluate \
    --output_dir ./output/internvl3_eval \
    --use_wandb
```

### Two-Stage Training Approach

#### Training
```bash
# Automated two-stage training
python train_two_stage.py \
    --output_dir ./output/two_stage_training \
    --use_wandb

# Manual Stage 1 training
python internvl3_bbox.py \
    --config configs/internvl3_stage1.yaml \
    --output_dir ./output/stage1 \
    --use_wandb

# Manual Stage 2 training (requires Stage 1 checkpoint)
python internvl3_bbox.py \
    --config configs/internvl3_stage2.yaml \
    --output_dir ./output/stage2 \
    --checkpoint ./output/stage1/checkpoint_2.pth \
    --use_wandb
```

#### Evaluation
```bash
# Evaluate Stage 1 model
python internvl3_bbox.py \
    --config configs/internvl3_stage1.yaml \
    --checkpoint ./output/stage1/checkpoint_2.pth \
    --evaluate \
    --output_dir ./output/stage1_eval

# Evaluate Stage 2 model (final model)
python internvl3_bbox.py \
    --config configs/internvl3_stage2.yaml \
    --checkpoint ./output/stage2/checkpoint_4.pth \
    --evaluate \
    --output_dir ./output/stage2_eval \
    --use_wandb
```

### Comparative Analysis

#### Performance Comparison
```bash
# Compare single-stage vs two-stage
python eval_compare_baselines.py \
    --single_stage_checkpoint ./output/internvl3_single_stage/checkpoint_4.pth \
    --two_stage_checkpoint ./output/stage2/checkpoint_4.pth \
    --output_dir ./output/comparison

# Benchmark evaluation
python benchmark_evaluation.py \
    --checkpoints ./output/internvl3_single_stage/checkpoint_4.pth ./output/stage2/checkpoint_4.pth \
    --labels "Single-Stage" "Two-Stage" \
    --output_dir ./output/benchmark
```

#### Training Efficiency Analysis
```bash
# Analyze training logs
python analyze_training_efficiency.py \
    --single_stage_log ./output/internvl3_single_stage/log.txt \
    --two_stage_log ./output/two_stage_training/experiment_summary.json \
    --output_dir ./output/efficiency_analysis
```

### Evaluation Metrics

The evaluation framework measures:

1. **Cross-modal Retrieval**: R@1, R@5, R@10 for both image-to-text and text-to-image
2. **Bounding Box Accuracy**: IoU, GIoU, L1 loss
3. **Spatial Relations**: Classification accuracy, F1 score
4. **Training Efficiency**: Epochs to convergence, training time
5. **Resource Usage**: GPU memory, parameter efficiency

### Expected Performance Comparison

| Metric | Single-Stage | Two-Stage | Improvement |
|--------|-------------|-----------|-------------|
| R@1 (I2T) | 65-68% | 70-73% | +5% |
| R@5 (I2T) | 85-88% | 88-91% | +3% |
| R@10 (I2T) | 92-94% | 94-96% | +2% |
| Training Epochs | 12-15 | 8 | 40-47% faster |
| Convergence Time | 8-10 hours | 6 hours | 25-40% faster |
| Memory Usage | ~18GB | ~16GB | 11% reduction |

### Methodology References

For detailed implementation of the two-stage training approach, see:
- `methodology_docs/internvl3_two_stage_methodology.md` - Complete two-stage training methodology
- `configs/internvl3_stage1.yaml` - Stage 1 configuration
- `configs/internvl3_stage2.yaml` - Stage 2 configuration  
- `train_two_stage.py` - Automated two-stage training script
- `models/model_internvl3_two_stage.py` - Enhanced two-stage model implementation